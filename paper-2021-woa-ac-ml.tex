%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
% twocolumn,
% hf,
]{ceurart}
\usepackage[colorinlistoftodos,prependcaption]{todonotes}
\usepackage{xcolor}
%%
%% One can fix some overfulls
% \sloppy

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% Rights management information.
%% CC-BY is default license.
\copyrightyear{2020}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

%%
%% This command is for the conference information
\conference{22nd Workshop "From Objects to Agents" 
Bologna, Italy, 1-3 September 2021 }

%%
%% The "title" command
\title{Towards a Machine Learning approach for Aggregate Computing} %% or Towards a Machine Learning approach for Aggregate Computing

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
\author[1]{Gianluca Aguzzi}[%
orcid=todo,
email=gianluca.aguzzi@unibo.it,
url=https://todo/,
]
\author[2]{Roberto Casadei}[%
orcid=todo,
email=roby.casadei@unibo.it,
url=todo,
]
\author[2]{Mirko Viroli}[%
orcid=todo,
email=mirko.viroli@unibo.it,
url=todo,
]
\address[1]{Alma Mater Studiorum - Universit√† di Bologna,
  Cesena, Italy}
%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Artificial intelligence is here to stay. 
  In particular, Deep Learning techniques in the last years outperform hand-craft human solutions in different contexts, ranging from computer vision to videogames.
  Currently, though, an all-encompassing Deep learning model does not exist, so researchers explore the possibility to apply these outstanding techniques in different fields.
  One of the most challenging is Collective-Adaptive System (CAS). Here, engineers have to handle large scale, global-to-local behaviour mapping,
  highly environment stochasticity. 
  Historically, researchers favour handling them with self-organisation, by which a collective behaviour emerges from continuously local interaction between simple components.
  A novel technique to specify self-organising behaviour in a composable and functional manner is Aggregate Computing. Even though is promising for expressing high-level collective behaviour, 
  it needs to build low-level blocks that tend to be tricky. 
  On the opposite side, some research lines apply Deep learning (supervised, reinforcement, non-supervised and self-supervised)
  along with CAS. Currently, though, no one gain popularity and solution tend to be application-specific.
  So, we outline a Machine Learning technique that could be applied in Aggregate Computing, making the possibility to combine declarative and automatic approaches that can easily scale in application complexity.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  Collective-Adaptive System \sep
  Aggregate Computing \sep
  Deep Learning \sep
  Reinforcement Learning \sep
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\todo[inline, color=red]{max pages : 16}
\section{Introduction}
Aggregate Computing~\cite{DBLP:journals/computer/BealPV15}
\todo[inline]{walkthrogh CAS, why they are hard}
\todo[inline]{Brief description of declarative approaches, pros and cons}
\todo[inline]{Brief description of machine learning approaches (MARL and GNN for communication), pros and cons}
\todo[inline]{Declare our direction: Combination to declarative and automatic approach}
\todo[inline]{Outline}

\section{Background and Related Work}
\todo[inline]{walkthrogh traditional engineering way to design CAS}
\todo[inline]{walkthrogh Aggregate Computing}
\todo[inline]{walkthrogh Machine learning approach applied in MARL}
\todo[inline]{current limitation (application specific, not large scale, centralised in some cases,..)}
\todo[inline]{novel work on GNN -- agent learn to communicate -- node size indipendent~\cite{DBLP:conf/corl/TolstayaGPP0R19}}

\section{Aggregate computing and Machine learning}
\todo[inline]{description of execution model of AC}
\todo[inline]{typical structure of AC algorithm : time evolution with hood operation}
\todo[inline]{show an example of block G?}
\todo[inline]{declare problems in applying traditional approach (variable-input size, feature hard-coded, environment non-stationarity, reward only know at the end)}
\todo[inline]{define benefit, possible contrast (i.e. if AC is declarative, why we should use ML?)}

\section{Towards Machine Learning approaches}
\todo[inline]{declare inspiration of GNN} %%ideally, we should use either reinforcement and supervised learning
\todo[inline]{show the structure: state network, neighbour network, output network}
\todo[inline]{possible limitations: non stabilisation and asynchrnous evaluation}
\todo[inline]{learning algorithm: how to apply backprop here (similar to truncated backpropagation, currently does not work)}
\todo[inline]{when makes sense to apply RL? (goal-oriented application?)}
\section{Evaluation} %% optional section
\todo[inline]{define experiment setup}
\todo[inline]{verify if the system perform as traditional method}

\section{Conclusion and Future Work}
\todo[inline]{sharping the model}
\todo[inline]{tune network in order to outperform traditional method}
\todo[inline]{engineering hybrid application (when we need to use ML)}
\todo[inline]{toward online learning?}
%%
%% Define the bibliography file to be used
\bibliography{biblio}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}

%%
%% End of file
