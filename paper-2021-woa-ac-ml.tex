%% The first command in your LaTeX source must be the \documentclass command.
%%
%% Options:
%% twocolumn : Two column layout.
%% hf: enable header and footer.
\documentclass[
  twocolumn,
  % hf
]{ceurart}
\usepackage[colorinlistoftodos,prependcaption]{todonotes}
\usepackage{xcolor}
\sloppy

\newcommand{\meta}[1]{{\color{blue}#1}}

\begin{document}
\conference{ - X - }

\title{Towards a Machine Learning approach for Aggregate Computing} %% or Towards a Machine Learning approach for Aggregate Computing

\author[1]{Gianluca Aguzzi}[%
 orcid=todo,
 email=gianluca.aguzzi@unibo.it,
 url=https://todo/,
]

\author[2]{Roberto Casadei}[%
 orcid=todo,
 email=roby.casadei@unibo.it,
 url=todo,
]

\author[2]{Mirko Viroli}[%
 orcid=todo,
 email=mirko.viroli@unibo.it,
 url=todo,
]
\address[1]{Alma Mater Studiorum - Universit√† di Bologna, Cesena, Italy}
%%%%% LEGENDA FOR NOTES %%%%%
%% RED: constraints
%% YELLOW: suggestions
%% CYAN: todos
%% GREEN: outline
\newcommand{\constraints}[1]{\todo[inline, color=red]{#1}}
\newcommand{\suggestions}[1]{\todo[inline, color=yellow]{#1}}
\newcommand{\todos}[1]{\todo[inline, color=cyan]{#1}}
\newcommand{\outline}[1]{\todo[inline, color=green]{#1}}

\begin{abstract}
Artificial intelligence is here to stay. 
 In particular, Deep Learning techniques in the last years outperform 
 hand-craft human solutions in different contexts, ranging from computer vision to videogames.
%
Currently, though, an all-encompassing Deep learning model does not exist, 
 so researchers explore the possibility to apply these techniques in different fields.

One of the most challenging is Collective-Adaptive System (CAS). % Or we want to use Cyber-Physical Swarm??
 Here, engineers have to handle large scale, global-to-local behaviour mapping,
 highly environment stochasticity. 
%
Historically, researchers favour handling them with self-organisation,
 by which a collective behaviour emerges from continuously local interaction between simple components.
%
A novel technique to specify self-organising behaviour 
 in a composable and functional manner is Aggregate Computing. 
 Even though it is promising for expressing high-level collective behaviour, 
 it needs to build low-level blocks that tend to be tricky. 

On the opposite side, some research lines apply Deep learning (supervised, reinforcement, non-supervised and self-supervised)
 along with CAS. Currently, though, no one gain popularity and solution tend to be application-specific.
%
So, we outline a Machine Learning technique that could be applied in Aggregate Computing, 
 making the possibility to combine specification-based and automatic approaches 
 that can easily scale in application complexity and to any system size.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\begin{keywords}
  Collective-Adaptive System \sep
  Aggregate Computing \sep
  Deep Learning \sep
  Reinforcement Learning \sep
\end{keywords}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.


\maketitle
\suggestions{Perhaps we could change the narration directly towards "reinforcement learning". I think that I cannot continue to explore multi solution..}
\section{Introduction}
Nowadays, computer engineers have to deal with Collective Adaptive Systems (CASs)
 due to the increasing interconnected computational devices placed in ever-changing environments.
 Examples of those include robot swarm, smart cities and augmenting crowds.

CASs are characterised by 
\begin{itemize}
\item a global order that emerges from local node interactions, 
\item  decentralised control, and 
\item adaptive behaviours towards environmental changes to pursuit a collective goal.
\end{itemize}
CAS are notoriously hard to design due as part of complex systems. 
%
So, over the years, in literature, various approaches aim at handling this complexity. 
%
Some of them take inspiration from the natural system
 composed of social animals --- like bees, fish and ants. 
%
Here, a sort of hive mind arises -- 
 something referred to as swarm intelligence -- 
born from local interactions between agents. 
%
This kind of behaviour is called self-organisation, 
 namely a global order that emerges from continuous 
 interactions between simple entities.

Initially, researchers try to bring those properties into  the computer science world 
 by miming the collective behaviour bottom-up, 
 namely trying to find the right single node behaviour 
 to reach a global behaviour, e.g. flocking, foraging, etc.
%
Though, this mapping isn't easily definable ---
 since we handle complex systems.
%
Ideally, we would like to express collective behaviours focusing only at the ensemble level,
 abstracting over non-functional aspects such as the network topology, environment uncertainties and 
 so on.

In this direction, Aggregate Computing is an innovative solution by which 
 developers can express collective self-organising behaviours through a functional program specification.
%
Indeed, an aggregate blueprint consists of manipulation of \textit{computational fields}, a distributed
 data structure where at each point in the spacetime is associated the result of a computation
 perform by a node.
%
These manipulations are defined in \textit{Field-calcus} --- the root of Aggregate Computing. 
 This calculus describes a set of minimal constructs to express any spatiotemporal computation.
%
Thanks to the ``composable'' nature of Aggregate Computing, we find common abstractions 
 in collective behaviour specification, and then we encapsulate them in so-called ``building-blocks''.
%
With those, different properties have been proven (such as \textit{eventual consistency}) and various
 applications have been build (such as swarm robotics, crowds engineering).

Anyway, building block synthesis concerns different stuff related not only to the behaviour itself,
 but also about the "ensemble" specifications. 
 For instance, it is quite difficult to craft building blocks
 that easily adapt w.r.t the node movements rapidity.

So, we claim that Machine Learning can help us in handling this "unseen" situation, leaving only the burden
 to developers of defining the right collective specifications.
%
This lead to a "hybrid" approach where a part of behaviour is still expressed using common 
 Aggregate Computing abstractions and another part is distilled through learning.
%
In this paper, as the first steps towards \textit{Hybrid Aggregate Computing}, we investigate how Aggregate Computing can be
 extended with Reinforcement Learning (and in particular with Independent Reinforcement Learning) through a case
 study where we outline the performance improvement of the "hop count" program.
 \suggestions{If we leave the part about "roadmap" and "machine learning similar setting", we need to add some description here.}
\suggestions{I do not know if we can avoid it altogether, but also a brief description of machine learning approaches can be usefull in the paper flow}
\todos{Outline}

\section{Background and Related Work}
\outline{Probably, for a "short" paper, this section can be avoided.}
\outline{walkthrogh traditional engineering way to design CAS}
\outline{walkthrogh Aggregate Computing}

\outline{I do not know, perhaps it is better to avoid this }
\outline{walkthrogh Machine learning approach applied in MARL}
\outline{current limitation (application-specific, not large scale, centralised in some cases,..)}

\section{Aggregate computing and Machine learning}

In this section,
 we analyse the characteristics of the aggregate computing paradigm,
 map these to relevant machine learning contributions in literature,
 pointing out different learning perspective about the integration of Aggregate Computing with Machine Learning,
 and summarise the analysis into a roadmap.

\paragraph{Multi- and many-agent system}
%
An aggregate system is a multi-agent system
 and, often, a \emph{many}-agent system
 where a large number (hundreds or more)
 of autonomous entities are programmed to achieve 
 some collective behaviour by means of \emph{repeated} 
 sensing, computation, communication, and actuation steps.
%
Due to the high stochasticity of the environment,
 it is almost impossible to know and
 program the optimal behaviour for all agents in advance.
 This results in the need to create intelligent agents
 so that they can learn the optimal behaviour and adapt to environmental changes.
%
In recent decades there has been an emerging trend in the use of Reinforcement Learning
 in multi-agent settings, as a powerful, robust and adaptive learning paradigm.
 Progress has been considerable and a wide range of algorithms are now available.
% to expand and and references
In general, there is not one technical solution in the multi-agent context.
 There are several (even orthogonal) viewpoints on which researchers have focused.
 One line of research (that of game theory) has tried to find techniques that achieve equilibrium such as Nash-Q learning~\cite{nash-q}.
 Others have focused on exploiting coordination between agents. 
 Others have applied single RL (such as Hysteretic Q-learning~\cite{hysteretic-q}) to each agent and achieved good results (even if no formal proof exist).
%
 The main problem with most of the solutions available in the literature is that they consider a small number of agents (or at least test them on small games).

\todos{present approaches for learning in MAS, e.g., through prominent surveys}

\paragraph{Neighbour-based or indirect (environment-mediated) interaction}
%
In aggregate systems,
 a device can only directly interact with neighbours.
%
Data flows may be implemented
 to support indirect communication across multiple hops;
 however, information from devices far away tends to be obsolete.
%
The environment can also be used, via stigmergy,
 for indirect communication;
 however, a device can typically only access 
  its very local portion of the environment.
%
In other words, the system state is only \emph{partially observable}.
%
\suggestions{What kind of ML algorithms can be used here?}
\paragraph{Learning goal: collective behaviour}
%
The devices of an aggregate 
 must cooperatively learn the ``aggregate program'',
 namely the collective behaviour 
 that achieves a particular \emph{global goal}
 in a decentralised, resilient way.
%

\paragraph{Self-organisation, swarm robotics}
%
\suggestions{ You can talk about similar problem structure (i.e. the collective follow the same local behaviour)}

\paragraph{Emergent behaviour}
%
The collective behaviour of an aggregate system
 \emph{emerges} 
 from a dense network of computations and interactions
 in an evolving environment.
%

\paragraph{Delayed, global rewards}
%
In the light of the above properties,
 if we consider a learning approach based on rewards,
 we must observe that such rewards would be
 delayed and mostly global---i.e.,
 it may not be easy to solve the credit-assignment problem
 to distinguish good from bad individual contributions
 to the global outcome.

\paragraph{Transient phase}
%
When an aggregate behaviour is carried out,
 we often focus on the stabilised outcomes,
 tolerating outcomes that are being generated
 during the transient phase.
%
However, intermediate results are often important,
 and invariants should hold throughout the entire computation.

\paragraph{Dynamic topology}
%
If we admit \emph{mobility} and \emph{failure},
 then we must also consider 
 the possibility of changes in neighbourhoods
 and hence the need of dealing with dynamic topologies.


\subsection{Intergration perspectives}
Here we outline the different perspectives about Hybrid Aggregate Computing with the aim
 to clarify the current research gap and to help us in the definition of a research roadmap.
%
\paragraph{Application} Learning can be useful at different levels in our framework.
 Surely, what for us is most important is to improve our building-block definition.
%
 Indeed, most of our results and applications are based on them.
 The building block improvement can succeed either by learning a correction factor 
 or by learning the behaviour entirely.

On the other side, learning can be useful also for framework related stuff,
 mainly aim at "non-functional" aspects. Trust mechanisms, evaluation frequency,
 energy consumption manager, package storage are all the things 
 that currently need to be handle by hand.
 
\suggestions{Move the definition of team learning and concurrent learning in the previous section}
\paragraph{Learning problem} As reference to \cite{mas-anc-cooperation}, Multi agent learning can be
 divided in \textit{concurrent} and \textit{team} learning. In the foster, each agent perform the 
 learning process and the in the latter, the learning processo is perform only by one agent an then 
 shared to the collective.
 Team learning is usually used in the ``swarm'' system, due to the homogeneous behaviour -- i.e. each
 agent perform the same program. Here, a common reward signal exist and evaluate the overall collective behaviour.
 Aggregate Computing is quite near to this kind of settings.
 On the other hand, Concurrent Learning, in \cite{csas-and-marl} seems to be usually applied in Collective
 Adaptive System, so again a field near to Aggregate Computing. In particular, Indipendedet Reinforcement Learning
 seems to reach good result in this kind of system, even if no thereom exist as the single agent context.
 Concurrent learning does not presume a global collective behaviour convernce. Indeed using a common reward signal,
 the collective can reach the same behaviour, as in the team learning settings as done here~\cite{iima2008swarm}.
\paragraph{Learning mode}
Aside of what kind of problem we setup, we can use Reinforcement Learning, Supervised Learning.
 Superivsed learning is quite rare in CSAS -- or in the swarm system in general. Novel work leverage Graph Neural network
 to learn how agent should communicate, supposing to know the right behaviour (i.e. produced via global vision simulation).
 In Aggregate Computing, Supervised Learning makes sense if we want to craft new building block by zero. 
 Indeed, we know eventually the right state of the system, but we do not know how the system can reach it

 Reinforcement Leanring is quite used in CAS, 
 due its flessibility and thanks to the plethora of apporaches used in this scenarion.
 In our scenario, Reinforcement Learning can be used both at the building block level and the framework level.
 For the forster, the simplest scenario can be that agents learn a correction factor to improve a long-term
 reward signal.
 
\paragraph{Learning time}
 Finally, learning can be perform offline, offline and then online, or fully-online.
 The foster is the most easy scenario. Here indeed, we can leverage simulation to know the right 
 collective value and then synthesis the right value. Supervised, Reinforcement Learning and even Evolutionary 
 computing can be used here. offline + online instead can be a mid-level complexity direction.
 Here, simulation can always leveraged to reach a good behavior, but then at online in can be
 adapted in front of new environmental situation. We know that it is quite problematic scenario. Reinforcement 
 Learning is the most suitable mode here --- also the "meta" learning can help.
 Finally, we can image that learning will be performed fully online. It is a common situation where
 we do not know the environment and also simulation can be very hard to accomplish. Here, again, Reinforcement learning
 seems to be the most suitable approach.

 
\todo[inline, color=green]{"application" pesprective: building block, framework, aggregate computing to guide learning}
\todo[inline, color=green]{learning perspective: homogeneous team learning}
\todo[inline, color=green]{learning mode perspective: offline through simulation, offline with local reward, offline then online, full-online}
\todo[inline, color=green]{method perspective: Supervised, reinforcement, multi-agent reinforcement}

\subsection{Roadmap}
\todo[inline, color=green]{AC + ML for building block refining -- offline}
\todo[inline, color=green]{AC + ML for building block refining -- online}
\todo[inline, color=green]{AC + ML for application refining -- offline/offline}
\todo[inline, color=green]{AC + ML for building block creation -- offline} %% why not?
\todo[inline, color=green]{AC + ML for building block creation -- online}
\todo[inline, color=green]{AC + ML for framework level -- online}

\section{Aggregate Computing with Reinforcement}
In this section, we describe our first result in Aggregate Computing and Machine learning combination,
 in particular, we briefly describe how nodes compute to follow the global specification,
 how Reinforcement Learning can be injected in the computation loop,
 and what kind of Reinforcement Learning can be used with Aggregate Computing.
\subsection{Computatational Model}
\subsection{Reinforcement Learning integration}
\subsection{Independent Reinforcement Learning}
As a first step, we investigate t
\section{Evaluation} %% optional section
\todo[inline]{define experiment setup, hop count problem}

\todo[inline]{verify if the system perform as traditional method}

\section{Conclusion and Future Work}
\todo[inline]{sharping the model}
\todo[inline]{tune network in order to outperform traditional method}
\todo[inline]{engineering hybrid application (when we need to use ML)}
\todo[inline]{toward online learning?}
%%
%% Define the bibliography file to be used
\nocite{*}
\bibliography{biblio}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\end{document}

%%
%% End of file
